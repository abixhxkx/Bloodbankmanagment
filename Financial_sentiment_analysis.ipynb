{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abixhxkx/Bloodbankmanagment/blob/main/Financial_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2ac28dd5-6eda-4a83-b66a-834b68af0a54",
      "metadata": {
        "id": "2ac28dd5-6eda-4a83-b66a-834b68af0a54"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import gc # pour optimiser la memoire"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wgxkuqggO9lQ"
      },
      "id": "wgxkuqggO9lQ",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv(\"/Users/yosrnoureddine/Downloads/all-data.csv\", delimiter=',', encoding='latin-1', header=None)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/all-data.csv\", delimiter=',', encoding='latin-1', header=None)\n",
        "df.columns = [\"Sentiment\", \"Message\"]\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aoMmmZO4WCUP",
        "outputId": "f0342730-8124-4507-9721-fae7fecaba95"
      },
      "id": "aoMmmZO4WCUP",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/all-data.csv'",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#df = pd.read_csv(\"/Users/yosrnoureddine/Downloads/all-data.csv\", delimiter=',', encoding='latin-1', header=None)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/content/drive/MyDrive/Colab Notebooks/all-data.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlatin-1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m df.columns = [\u001b[33m\"\u001b[39m\u001b[33mSentiment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMessage\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      4\u001b[39m df.head()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\dl\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\dl\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\dl\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\dl\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\dl\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/all-data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f8aa789-fcc2-47d3-8199-c16ed1ce3af8",
      "metadata": {
        "id": "2f8aa789-fcc2-47d3-8199-c16ed1ce3af8"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6618b91f-423e-439c-a73e-fcddb41a413c",
      "metadata": {
        "id": "6618b91f-423e-439c-a73e-fcddb41a413c"
      },
      "outputs": [],
      "source": [
        "cnt_pro = df['Sentiment'].value_counts()\n",
        "plt.figure(figsize=(12,4))\n",
        "sns.barplot(x=cnt_pro.index, y=cnt_pro.values, alpha=0.8)\n",
        "plt.ylabel('Number of Occurrences', fontsize=12)\n",
        "plt.xlabel('Sentiment', fontsize=12)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0254bff0-ec5e-46c3-be79-2ec5dfc09031",
      "metadata": {
        "id": "0254bff0-ec5e-46c3-be79-2ec5dfc09031"
      },
      "outputs": [],
      "source": [
        "#Convert string to numeric\n",
        "Sentiment  = {'positive': 0,'neutral': 1,'negative':2}\n",
        "\n",
        "df.Sentiment = [Sentiment[item] for item in df.Sentiment]\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9deca464-2cfe-4269-adc7-68d0b650f390",
      "metadata": {
        "id": "9deca464-2cfe-4269-adc7-68d0b650f390"
      },
      "outputs": [],
      "source": [
        "def print_message(index):\n",
        "    example = df[df.index == index][['Message', 'Sentiment']].values[0]\n",
        "    if len(example) > 0:\n",
        "        print(example[0])\n",
        "        print('Sentiment:', example[1])\n",
        "print_message(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "033a92cd-5a47-450e-ad63-a922fdf03d4b",
      "metadata": {
        "id": "033a92cd-5a47-450e-ad63-a922fdf03d4b"
      },
      "outputs": [],
      "source": [
        "print_message(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a60b7074-badb-42ab-b822-de1a3ef7dc17",
      "metadata": {
        "id": "a60b7074-badb-42ab-b822-de1a3ef7dc17"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def clean_financial_text(text):\n",
        "    \"\"\"\n",
        "    Cleans text for a FinBERT-style model.\n",
        "    Focuses on removing noise and normalizing, not over-processing.\n",
        "    \"\"\"\n",
        "    # 1. Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"lxml\").text\n",
        "\n",
        "    # 2. Replace URLs with a special <URL> token\n",
        "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
        "\n",
        "\n",
        "    # 3. Normalize stock tickers.\n",
        "    # We add a space to help the tokenizer. '$AAPL' -> ' $AAPL '\n",
        "    # The model will learn what $ means in context.\n",
        "    # An alternative: re.sub(r'\\$([A-Z]+)', r'ticker \\1', text)\n",
        "    text = re.sub(r'(\\$[A-Z]+)', r' \\1 ', text)\n",
        "\n",
        "    # 4. Normalize numbers (optional but can help)\n",
        "    # This groups all numbers into one token.\n",
        "    # e.g., \"profit of 1,000,000\" -> \"profit of <NUM>\"\n",
        "    # text = re.sub(r'(\\d+[\\d.,]*%?)', r' <NUM> ', text)\n",
        "\n",
        "    # 5. Remove any specific artifacts (like your |||)\n",
        "    text = re.sub(r'\\|\\|\\|', r' ', text)\n",
        "\n",
        "    # 6. Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # --- THINGS WE DO NOT DO ---\n",
        "    #\n",
        "    # We DO NOT convert to lowercase (let the \"uncased\" tokenizer handle it).\n",
        "    # We DO NOT remove punctuation (BERT understands \"!\" and \"?\").\n",
        "    # We DO NOT remove stop words (\"not good\" is very different from \"good\").\n",
        "    # We DO NOT perform stemming/lemmatization (BERT understands \"grew\" vs \"growing\").\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example usage:\n",
        "# df['Message'] = df['Message'].apply(clean_financial_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf887dca-5c7d-442e-a883-579b84b8a4d5",
      "metadata": {
        "id": "cf887dca-5c7d-442e-a883-579b84b8a4d5"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df['Message'] = df['Message'].apply(clean_financial_text)\n",
        "train, test = train_test_split(df, test_size=0.1 , random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b58ea0ce-4631-4b3e-9e09-0fad4491d229",
      "metadata": {
        "id": "b58ea0ce-4631-4b3e-9e09-0fad4491d229"
      },
      "outputs": [],
      "source": [
        "# Display dataset size and first samples\n",
        "print(\"Total number of rows:\", len(df))\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c2626c7-239d-4c6d-8ef2-45c19c50042e",
      "metadata": {
        "id": "6c2626c7-239d-4c6d-8ef2-45c19c50042e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count samples per sentiment class, this helps us see if the dataset is balanced or not.\n",
        "\n",
        "sentiment_counts = df['Sentiment'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(sentiment_counts.index, sentiment_counts.values)\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Number of samples\")\n",
        "plt.title(\"Sentiment distribution\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5da2dd58-5fa5-4b83-a97d-65e2a53c488c",
      "metadata": {
        "id": "5da2dd58-5fa5-4b83-a97d-65e2a53c488c"
      },
      "outputs": [],
      "source": [
        "# Compute number of words in each message short or very long messages can affect model performance.\n",
        "\n",
        "df['length'] = df['Message'].apply(lambda x: len(x.split()))\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(df['length'], bins=50)\n",
        "plt.xlabel(\"Number of words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Message length distribution\")\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics on message length\n",
        "print(df['length'].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e54ec520-3a17-4d8d-a51a-8106a68d3e4f",
      "metadata": {
        "id": "e54ec520-3a17-4d8d-a51a-8106a68d3e4f"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Tokenization: extract word tokens (simple regex tokenizer) it gives an idea of the vocabulary used in financial texts.\n",
        "def tokenize(text):\n",
        "    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
        "    return tokens\n",
        "\n",
        "# Apply tokenization\n",
        "df['tokens'] = df['Message'].apply(tokenize)\n",
        "\n",
        "# Flatten all tokens into one list\n",
        "all_words = [word for tokens in df['tokens'] for word in tokens]\n",
        "word_counts = Counter(all_words)\n",
        "\n",
        "print(\"Top 20 most frequent words:\")\n",
        "print(word_counts.most_common(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task\n",
        "Generate a word cloud visualization from the `word_counts` object, and then provide a summary of the generated word cloud, highlighting prominent words and their potential significance in the financial context."
      ],
      "metadata": {
        "id": "lPoTMzlaETAh"
      },
      "id": "lPoTMzlaETAh"
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"WordCloud and matplotlib.pyplot imported successfully.\")"
      ],
      "metadata": {
        "id": "KlAaWdNEEQ7k"
      },
      "id": "KlAaWdNEEQ7k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(width = 800, height = 400,\n",
        "                background_color ='white',\n",
        "                min_font_size = 10).generate_from_frequencies(word_counts)\n",
        "\n",
        "# Display the generated image:\n",
        "plt.figure(figsize = (10, 5), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"Word Cloud generated and displayed successfully.\")"
      ],
      "metadata": {
        "id": "9mrn4ev4E21q"
      },
      "id": "9mrn4ev4E21q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64921ed9-b2fe-4c57-951a-ef4044435963",
      "metadata": {
        "id": "64921ed9-b2fe-4c57-951a-ef4044435963"
      },
      "outputs": [],
      "source": [
        "print(\"Example positive message:\\n\", df[df.Sentiment==0].Message.iloc[0])\n",
        "print(\"\\nExample neutral message:\\n\", df[df.Sentiment==1].Message.iloc[0])\n",
        "print(\"\\nExample negative message:\\n\", df[df.Sentiment==2].Message.iloc[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "gp5mwZCkO8Zf"
      },
      "id": "gp5mwZCkO8Zf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f369c0-5ac6-48c8-8e6f-f66b0fed8033",
      "metadata": {
        "id": "14f369c0-5ac6-48c8-8e6f-f66b0fed8033"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First split: Train (80%) + Temp (20%)\n",
        "train_df, temp_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['Sentiment'] # keeps class balance\n",
        ")\n",
        "\n",
        "# Second split: Validation (10%) + Test (10%)\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    stratify=temp_df['Sentiment']\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(train_df))\n",
        "print(\"Validation size:\", len(val_df))\n",
        "print(\"Test size:\", len(test_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a86f5fbc-8278-46c1-8efb-f32d47db68b5",
      "metadata": {
        "id": "a86f5fbc-8278-46c1-8efb-f32d47db68b5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fecf19c"
      },
      "source": [
        "The first step is to install the `transformers` library as requested in the instructions.\n",
        "\n"
      ],
      "id": "1fecf19c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f7fbdff"
      },
      "source": [
        "#pip install transformers"
      ],
      "id": "1f7fbdff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9e631df"
      },
      "source": [
        "The next step is to install the `torch` library, as specified in the instructions.\n",
        "\n"
      ],
      "id": "a9e631df"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ae4c51c"
      },
      "source": [
        "#pip install torch"
      ],
      "id": "4ae4c51c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d122bf2"
      },
      "source": [
        "print(\"Cleaned 'Message' column for train_df:\")\n",
        "print(train_df[['Message']].head())\n",
        "print(\"\\nCleaned 'Message' column for val_df:\")\n",
        "print(val_df[['Message']].head())\n",
        "print(\"\\nCleaned 'Message' column for test_df:\")\n",
        "print(test_df[['Message']].head())"
      ],
      "id": "7d122bf2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c704638"
      },
      "source": [
        "## Load Tokenizer and Prepare Data\n",
        "\n",
        "Load the FinBERT tokenizer and use it to tokenize and encode the 'Message' column of your `train_df`, `val_df`, and `test_df`. Convert the tokenized data and sentiment labels into PyTorch TensorDatasets and DataLoaders, which are required for model training.\n"
      ],
      "id": "9c704638"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cc07737"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to import the `AutoTokenizer` class from the `transformers` library and then load the pre-trained FinBERT tokenizer using `AutoTokenizer.from_pretrained()`, as specified in the instructions.\n",
        "\n"
      ],
      "id": "3cc07737"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21099081"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Load the FinBERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
        "\n",
        "print(\"FinBERT tokenizer loaded successfully.\")"
      ],
      "id": "21099081",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1981a291"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully loaded the FinBERT tokenizer. Now, I will proceed to tokenize and encode the 'Message' column for the train, validation, and test dataframes using the loaded tokenizer. Then, I will convert the tokenized data and sentiment labels into PyTorch TensorDatasets and DataLoaders, as specified in the instructions.\n",
        "\n"
      ],
      "id": "1981a291"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V4q4vUyR9gqb"
      },
      "id": "V4q4vUyR9gqb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5913c799"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "def encode_data(tokenizer, dataframe):\n",
        "    messages = dataframe['Message'].tolist()\n",
        "    labels = dataframe['Sentiment'].tolist()\n",
        "\n",
        "    # Tokenize and encode messages\n",
        "    encoded_inputs = tokenizer(\n",
        "        messages,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoded_inputs['input_ids']\n",
        "    attention_mask = encoded_inputs['attention_mask']\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return TensorDataset(input_ids, attention_mask, labels_tensor)\n",
        "\n",
        "# Encode datasets\n",
        "train_dataset = encode_data(tokenizer, train_df)\n",
        "val_dataset = encode_data(tokenizer, val_df)\n",
        "test_dataset = encode_data(tokenizer, test_df)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"Data tokenization, encoding, and DataLoader creation completed.\")\n",
        "print(f\"Train DataLoader has {len(train_dataloader)} batches.\")\n",
        "print(f\"Validation DataLoader has {len(val_dataloader)} batches.\")\n",
        "print(f\"Test DataLoader has {len(test_dataloader)} batches.\")"
      ],
      "id": "5913c799",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5bc7892"
      },
      "source": [
        "## Load FinBERT Model for Sequence Classification\n",
        "\n",
        "### Subtask:\n",
        "Load the pre-trained FinBERT model for sequence classification from the `transformers` library. Configure the model with the correct number of output labels (corresponding to our sentiment categories).\n"
      ],
      "id": "d5bc7892"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1598413"
      },
      "source": [
        "\n",
        "The first step is to import the `AutoModelForSequenceClassification` class from the `transformers` library, determine the number of unique sentiment labels, and then load the pre-trained FinBERT model for sequence classification, as specified in the instructions. This prepares the model for further training or evaluation.\n",
        "\n"
      ],
      "id": "a1598413"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72d9655a"
      },
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Determine the number of unique sentiment labels\n",
        "num_labels = df['Sentiment'].nunique()\n",
        "\n",
        "# Load the pre-trained FinBERT model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert', num_labels=num_labels)\n",
        "\n",
        "print(f\"FinBERT model loaded with {num_labels} output labels.\")"
      ],
      "id": "72d9655a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acb314ee"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully loaded the FinBERT model. Now, I need to define the computation device (GPU if available, otherwise CPU) and move the loaded model to this device, as instructed.\n",
        "\n"
      ],
      "id": "acb314ee"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e6b3cae"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model moved to device: {device}\")"
      ],
      "id": "2e6b3cae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9dd0c40"
      },
      "source": [
        "## Define Training Parameters and Functions\n",
        "\n",
        "### Subtask:\n",
        "Set up the device (GPU if available), define the optimizer (e.g., AdamW) and a learning rate scheduler. Implement helper functions for a single training epoch and for evaluating the model on a given dataset.\n"
      ],
      "id": "d9dd0c40"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd6bd602"
      },
      "source": [
        "\n",
        "The first step is to import the `AdamW` optimizer from `transformers.optimization` and the `get_linear_schedule_with_warmup` function from `transformers.get_scheduler` to prepare for defining the optimizer and scheduler.\n",
        "\n"
      ],
      "id": "bd6bd602"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e883ec22"
      },
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "\n",
        "print(\"Imported AdamW from torch.optim and get_scheduler from transformers.\")"
      ],
      "id": "e883ec22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ba31383"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the necessary imports are done, I will define the `AdamW` optimizer for the model's parameters with a learning rate of 2e-5, and then define the learning rate scheduler using `get_linear_schedule_with_warmup` as specified in the instructions. This requires calculating the total number of training steps.\n",
        "\n"
      ],
      "id": "1ba31383"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf2af362"
      },
      "source": [
        "learning_rate = 5e-5\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "num_epochs = 3 # Example number of epochs, you can adjust this\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "\n",
        "scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "print(f\"Optimizer initialized with learning rate: {learning_rate}\")\n",
        "print(f\"Learning rate scheduler initialized for {total_steps} total training steps.\")"
      ],
      "id": "cf2af362",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca293a22"
      },
      "source": [
        "\n",
        "Now that the optimizer and scheduler are defined, the next step is to implement the `train_epoch` helper function, as specified in the instructions. This function will encapsulate the logic for a single pass through the training data, handling forward and backward propagation, loss calculation, and parameter updates.\n",
        "\n"
      ],
      "id": "ca293a22"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "680fe992"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss\n",
        "\n",
        "print(\"train_epoch function defined.\")"
      ],
      "id": "680fe992",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7489c09c"
      },
      "source": [
        "**Reasoning**:\n",
        "The `train_epoch` function has been defined. Now, I need to implement the `evaluate_model` helper function to assess the model's performance on a given dataset, as specified in the instructions. This function will calculate the average loss and accuracy.\n",
        "\n"
      ],
      "id": "7489c09c"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RvGQIBRL9nXP"
      },
      "id": "RvGQIBRL9nXP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef30753d"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def evaluate_model(model, dataset, device, batch_size=16):\n",
        "    model.eval()  # Mode évaluation\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_true_labels = []\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    with torch.no_grad():  # Pas de calcul de gradients\n",
        "        for batch in dataloader:\n",
        "            # Si batch est tuple (input_ids, attention_mask, labels)\n",
        "            # Sinon adapter ce déballage selon structure de batch\n",
        "            if isinstance(batch, dict) or hasattr(batch, 'keys'):\n",
        "                inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
        "                labels = batch['label'].to(device)\n",
        "            else:\n",
        "                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "                inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
        "\n",
        "            outputs = model(**inputs, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "            true_labels = labels.cpu().numpy()\n",
        "\n",
        "            all_predictions.extend(preds)\n",
        "            all_true_labels.extend(true_labels)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(all_true_labels, all_predictions)\n",
        "    f1 = f1_score(all_true_labels, all_predictions, average='weighted')\n",
        "\n",
        "    return {'loss': avg_loss, 'accuracy': accuracy, 'f1_score': f1}\n"
      ],
      "id": "ef30753d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device setup (force CPU for lightweight requirement)\n",
        "device = torch.device('cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "1WWVggj5Bqcd"
      },
      "id": "1WWVggj5Bqcd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nge09DweCPrQ"
      },
      "id": "Nge09DweCPrQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dce99ddd"
      },
      "source": [
        "## Train the FinBERT Model\n",
        "\n",
        "### Subtask:\n",
        "Execute the training loop for multiple epochs. In each epoch, train the model on the training data and evaluate its performance on the validation data to monitor progress and prevent overfitting.\n"
      ],
      "id": "dce99ddd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7febb075"
      },
      "source": [
        "Now that the `train_epoch` and `evaluate_model` functions are defined, I will execute the main training loop. This loop will iterate for a predefined number of epochs, training the model on the training dataset and evaluating its performance on the validation dataset in each epoch. I will track and print the training loss, validation loss, and validation accuracy to monitor the model's progress and detect potential overfitting."
      ],
      "id": "7febb075"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfc2be1d"
      },
      "source": [
        "**Reasoning**:\n",
        "I will initialize variables to store the training history, then execute the main training loop for the specified number of epochs, calling the `train_epoch` and `evaluate_model` functions, and printing the loss and accuracy for each epoch as instructed.\n",
        "\n"
      ],
      "id": "cfc2be1d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36da435b"
      },
      "source": [
        "# Initialize metric tracking lists\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "val_f1_scores = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    avg_train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    eval_results = evaluate_model(model, val_dataloader, device)\n",
        "    val_losses.append(eval_results['loss'])\n",
        "    val_accuracies.append(eval_results['accuracy'])\n",
        "    val_f1_scores.append(eval_results['f1_score'])\n",
        "\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"Validation Loss: {eval_results['loss']:.4f}\")\n",
        "    print(f\"Validation Accuracy: {eval_results['accuracy']:.4f}\")\n",
        "    print(f\"Validation F1 Score: {eval_results['f1_score']:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n"
      ],
      "id": "36da435b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let test our model on unknown data (test set)\n",
        "final_results = evaluate_model(model, test_dataset, device)\n",
        "print(\"Performance finale sur test :\", final_results)\n"
      ],
      "metadata": {
        "id": "4CXEsy0iAY7H"
      },
      "id": "4CXEsy0iAY7H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DjmtV9XqAkti"
      },
      "id": "DjmtV9XqAkti",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINE TUNING PART"
      ],
      "metadata": {
        "id": "mfIXGnmPAoHy"
      },
      "id": "mfIXGnmPAoHy"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}